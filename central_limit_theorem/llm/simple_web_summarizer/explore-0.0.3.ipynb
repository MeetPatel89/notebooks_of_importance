{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "from ml_boilerplate_module.config import load_config\n",
    "from ml_boilerplate_module.llm.brochure import create_brochure\n",
    "from ml_boilerplate_module.llm.client_factory import get_llm_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://huggingface.co/\"\n",
    "# company_name = \"Hugging Face\"\n",
    "\n",
    "url = \"https://anthropic.com/\"\n",
    "company_name = \"Anthropic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create brochure using OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = create_brochure(url=url, provider=\"opeanai\", model=\"\", company_name=company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result.split(\"```markdown\")[1].split(\"```\")[0].strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create brochure using Ollama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = create_brochure(url=url, provider=\"ollama\", model=\"gemma3\", company_name=company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result2.split(\"```markdown\")[1].split(\"```\")[0].strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradio example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_message(system_message: str, user_message: str, provider: str, model: str) -> str:\n",
    "    client = get_llm_client(provider=provider, model=model)\n",
    "    return client.send_message(system_message=system_message, user_message=user_message)\n",
    "\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=stream_message,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"System Message\"),\n",
    "        gr.Textbox(label=\"User Message\"),\n",
    "        gr.Dropdown(label=\"Provider\", choices=[\"openai\", \"anthropic\"]),\n",
    "        gr.Dropdown(label=\"Model\", choices=[\"gpt-4o-mini\", \"gpt-4.1\", \"claude-3-5-sonnet-20240620\"]),\n",
    "    ],\n",
    "    outputs=[gr.Markdown(label=\"Response\")],\n",
    "    flagging_mode=\"never\",\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_llm_client(provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\")\n",
    "send_message = client.send_message\n",
    "stream_message = client.stream_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_llm_client(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "client.send_message(\n",
    "    system_message=\"You are a helpful assistant that responds in markdown.\",\n",
    "    user_message=\"What is today's date?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_message,\n",
    "    inputs=[gr.Textbox(label=\"System Message\"), gr.Textbox(label=\"User Message\")],\n",
    "    outputs=[gr.Markdown(label=\"Response\")],\n",
    "    flagging_mode=\"never\",\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from ml_boilerplate_module import load_config\n",
    "from ml_boilerplate_module.llm.anthropic_client import AnthropicAIClient\n",
    "from ml_boilerplate_module.llm.chat import Chatbot\n",
    "from ml_boilerplate_module.llm.google_client import GoogleAIClient\n",
    "from ml_boilerplate_module.llm.openai_client import OpenAIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_client = GoogleAIClient(model=\"gemini-2.0-flash\")\n",
    "chatbot = Chatbot(client=google_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI model: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "openai_client = OpenAIClient()\n",
    "chatbot = Chatbot(openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_client = AnthropicAIClient()\n",
    "chatbot = Chatbot(anthropic_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are a machine learning professor proficient in teaching graduate level students\n",
    "in a top American university. You are good at using detailed mathematical concepts\n",
    "and notations along with examples to ensure students understand machine learning deeply.\n",
    "Response should be in markdown format.\n",
    "\"\"\"\n",
    "\n",
    "# user_message = \"\"\"Explain regularization in logistic regression.\n",
    "# Feel free to dive into as much detail as required to explain the concept\n",
    "# at masters or PhD level.\"\"\"\n",
    "\n",
    "# chatbot.add_message(role=\"system\", content=system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbot.send_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(message, history):\n",
    "    print(\"Message: \", message)\n",
    "    print(\"History: \", history)\n",
    "    response = chatbot.stream_message(user_message=message, system_message=system_message)\n",
    "    full_response = \"\"\n",
    "    for chunk in response:\n",
    "        full_response += chunk\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message(message, history):\n",
    "    print(\"Message from gradio: \")\n",
    "    print(message)\n",
    "    print(\"History from gradio: \")\n",
    "    print(history)\n",
    "    response = chatbot.send_message(user_message=message, system_message=system_message)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=message, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
